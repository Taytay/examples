name: llama2-70b
compute:
  gpus: 8
  instance: inference.oci.bm.gpu.b4.8
image: mosaicml/inference:0.1.16
replicas: 1
command: |
  export PYTHONPATH=$PYTHONPATH:/code/examples
integrations:
- integration_type: git_repo
  git_repo: mosaicml/examples
  ssh_clone: false
model:
  download_parameters:
    hf_path: meta-llama/Llama-2-70b-chat-hf
  model_handler: examples.inference-deployments.llama2.llama2_handler.Llama2ModelHandler
  model_parameters:
    model_name: meta-llama/Llama-2-70b-chat-hf
